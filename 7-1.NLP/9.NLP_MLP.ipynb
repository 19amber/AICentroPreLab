{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from konlpy.tag import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All string : 나는 너를 사랑한다. 나는 나를 사랑한다. 나는 나를 증오한다. 나는 너를 증오한다. 너는 너를 증오한다.\n",
      "All tockents: ['나', '너', '사랑', '나', '나', '사랑', '나', '나', '증오', '나', '너', '증오', '너', '너', '증오']\n",
      "Dictionary :  {'나': 0, '너': 1, '사랑': 2, '증오': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>나는 너를 사랑한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나는 나를 사랑한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>나는 나를 증오한다.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>나는 너를 증오한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>너는 너를 증오한다.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       corpus  label  0  1  2  3\n",
       "0   0  나는 너를 사랑한다.      1  1  1  1  0\n",
       "1   1  나는 나를 사랑한다.      0  2  0  1  0\n",
       "2   2  나는 나를 증오한다.      0  2  0  0  1\n",
       "3   3  나는 너를 증오한다.      1  1  1  0  1\n",
       "4   4  너는 너를 증오한다.      1  0  2  0  1"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus= [\n",
    "    \"나는 너를 사랑한다.\",\n",
    "    \"나는 나를 사랑한다.\",\n",
    "    \"나는 나를 증오한다.\",\n",
    "    \"나는 너를 증오한다.\",\n",
    "    \"너는 너를 증오한다.\"\n",
    "    ]\n",
    "df=pd.DataFrame({\"id\": range(len(corpus)),\"corpus\": corpus,\"label\":[1,0,0,1,1]\n",
    "})                 \n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "s=\" \".join(df.corpus)\n",
    "\n",
    "print('All string :' ,s)\n",
    "\n",
    "# posToUse=[\"NNP\",\"NNG\",\"MAG\",\"NP\",\"VV\",\"VV+EF\",\"IC\"]\n",
    "posToUse=[\"NNP\",\"NNG\",\"NP\",\"NNB\"]\n",
    "\n",
    "def getTokens(s):\n",
    "    global posToUse\n",
    "    return [ i[0] for i in  Mecab().pos(s) if i[1] in posToUse ] \n",
    "    \n",
    "print('All tockents:', getTokens(s))\n",
    "\n",
    "posToUse=[\"NNP\",\"NNG\",\"NP\",\"NNB\"]\n",
    "stopwords=[\"놈\",\"18\"]\n",
    "def getToken(s,pos=posToUse,stopword=stopwords):\n",
    "    return [ w for w,t in  Mecab().pos(s) if t in pos and w not in stopword ] \n",
    "vect = CountVectorizer(tokenizer=getToken)\n",
    "vect.fit(getTokens(s))\n",
    "print('Dictionary : ',vect.vocabulary_)\n",
    "cd=pd.DataFrame(vect.transform([\" \".join(getTokens(i)) for i in df.corpus]).toarray())\n",
    "data=pd.concat([df,cd], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "x_train=data.iloc[:,3:11].to_numpy()\n",
    "#y_train=data.label.to_numpy().reshape([-1,1])\n",
    "y_train=keras.utils.to_categorical(data.label,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_136 (Dense)            (None, 14)                70        \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 14)                210       \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 4)                 60        \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 350\n",
      "Trainable params: 350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(14, activation='tanh', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dense(14, activation='tanh'))\n",
    "model.add(Dense(4, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5 samples, validate on 5 samples\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 296ms/sample - loss: 0.8330 - acc: 0.4000 - val_loss: 0.8032 - val_acc: 0.6000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 6ms/sample - loss: 0.7970 - acc: 0.6000 - val_loss: 0.7728 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 5ms/sample - loss: 0.7638 - acc: 0.6000 - val_loss: 0.7449 - val_acc: 0.6000\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 0.7363 - acc: 0.6000 - val_loss: 0.7179 - val_acc: 0.6000\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 0.7107 - acc: 0.6000 - val_loss: 0.6928 - val_acc: 0.6000\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 0.6825 - acc: 0.6000 - val_loss: 0.6688 - val_acc: 0.6000\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 6ms/sample - loss: 0.6593 - acc: 0.6000 - val_loss: 0.6452 - val_acc: 0.6000\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 0.6371 - acc: 0.6000 - val_loss: 0.6218 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 4ms/sample - loss: 0.6136 - acc: 0.6000 - val_loss: 0.5993 - val_acc: 0.6000\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 3ms/sample - loss: 0.5948 - acc: 0.6000 - val_loss: 0.5768 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5767619013786316\n",
      "Train accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3727829 , 0.62721705],\n",
       "       [0.46163276, 0.53836715],\n",
       "       [0.4542633 , 0.5457367 ],\n",
       "       [0.36648417, 0.63351583],\n",
       "       [0.32888445, 0.67111546]], dtype=float32)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
